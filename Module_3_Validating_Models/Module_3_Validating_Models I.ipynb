{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pd6-VDYOqv56"
   },
   "source": [
    "<span style=\"font-family: Palatino; font-size: 40px; color: purple\">\n",
    "             Measuring Classifier Performance - I\n",
    "</span>\n",
    "\n",
    "\n",
    "Spring 2022 - Instructors: Roger Stein and Ben Wolfson -\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "\n",
    "    <STYLE type=\"text/css\">\n",
    "       H1 {font-family: Palatino; font-size: 30px; color: purple}\n",
    "    </STYLE>\n",
    "\n",
    "\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we are going to be learning about how to evaluate a model's performance. But before diving into the nitty gritty, let's try to understand why we need to evaluate a model in the first place. Why can't we just build a model on our data, get it as close to 100% accurate as possible and continue on to the next project?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to explore this topic, we turn back to polynomial regressions, before looking at an example on a real dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and houskeeping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libaraies we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import copy as cp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from icecream import ic\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='ticks', palette='Set2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have not installed `pandas` or `dataprep`, run this chunk\n",
    "\n",
    "#!pip install -U dataprep\n",
    "#!pip install pandas==1.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataprep.datasets import load_dataset\n",
    "from dataprep.eda import create_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For data description and transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_classes(x):\n",
    "    N = df.shape[0]\n",
    "    m = np.sum(x)\n",
    "    n = N - m\n",
    "    print(\"{:>10,}\".format(N), \"records in total.\")\n",
    "    print(\"{:>10,}\".format(m) + \" positive (1) instances.\")\n",
    "    print(\"{:>10,}\".format(n) + \" negative (0) instances.\")\n",
    "    return N, m, n\n",
    "\n",
    "def discretize_Y(Y, threshold):\n",
    "    Y = pd.DataFrame(Y)\n",
    "    Y = Y > threshold\n",
    "    return Y\n",
    "\n",
    "def print_confusion_matrix(M, nclass = 2):\n",
    "    col_names = M.columns\n",
    "    ncol      = len(col_names)\n",
    "    row_names = M.index\n",
    "    nrow      = len(row_names)\n",
    "    \n",
    "    print('{:>8}'.format(''), end = '')\n",
    "    for j in range(ncol):  # horiz header\n",
    "        print('{:>8}'.format(col_names[j]), end = '')\n",
    "    print('')\n",
    "        \n",
    "    for i in range(nrow):\n",
    "        print('{:>8}'.format(row_names[i]), end = '')\n",
    "        for j in range(ncol):\n",
    "            print ('{:>8,}'.format(M.iloc[i,j]),  end = '')\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For review of polynomial example (see \"Linear Models\" notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_color = ['blue', 'violet','red']\n",
    "\n",
    "def true_function(X):\n",
    "    return np.sin(1.5 * np.pi * X)\n",
    "\n",
    "\n",
    "def plot_example(X, Y, functions):\n",
    "    # Get some X's to plot the functions\n",
    "    X_test = pd.DataFrame(np.linspace(0, 1, 100), columns=['x1'])\n",
    "    # Plot stuff\n",
    "    for i,key in enumerate(functions):\n",
    "        plt.plot(X_test, functions[key](X_test), label=key, color=line_color[i],linewidth=3)\n",
    "    plt.scatter(X, Y, edgecolor='b', s=20, label=\"Samples\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((0, 1))\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.legend(loc=\"best\")\n",
    "    \n",
    "def fit_polynomial(X, Y, degree):\n",
    "    # create different powers of X\n",
    "    polynomial_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features), (\"linear_regression\", linear_regression)])\n",
    "    pipeline.fit(X, Y)\n",
    "    return pipeline\n",
    "\n",
    "def plot_poly(X, Y, degree):\n",
    "    # Fit polynomial model\n",
    "    model = fit_polynomial(X, Y, degree)\n",
    "    # Evaluate model\n",
    "    mse = mean_squared_error(Y, model.predict(X))\n",
    "    # Plot results\n",
    "    functions[\"Model\"] = model.predict\n",
    "    plt.title(\"Degree %d\\n MSE: %.2f\" % (degree, mse))\n",
    "    plot_example(X, Y, functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generally useful utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_bar(k, n, incr_txt=\"Step\", bar_len = 10):\n",
    "   \n",
    "    bar_char  = u'\\u25A5'\n",
    "    line_char = u'\\u21E2' # u'\\u2192'  u'\\u23AF' u'\\u25AD'\n",
    "    \n",
    "    places   = int(np.ceil(n/bar_len))\n",
    "    pct      = k/n\n",
    "    n_str    = \"{:,.0f}\".format(n)        \n",
    "    k_str    = \"{:,.0f}\".format(k)\n",
    "    pct_str  = \"{:,.0f}%\".format(k/n * 100)\n",
    "    # d_format = \"%\" + str(places) + \"d\"\n",
    "    \n",
    "    if k == n-1:\n",
    "        n_bars = bar_len\n",
    "        n_spaces = 0\n",
    "        text_txt = ('\\u25BB' * 10) + \" Completed \" + n_str + \" \" + incr_txt + \"s. \" + '\\u25C5' * 10\n",
    "    else:\n",
    "        n_bars   = int(np.floor(pct * bar_len))\n",
    "        n_spaces = bar_len - n_bars\n",
    "        text_txt = \" \" + pct_str +  \" (\" + incr_txt + \" \" + k_str + \" of  \" + n_str + \").\"\n",
    "\n",
    "    bar_txt  = \"[\" + \"\".ljust(n_bars,bar_char) + \"\".rjust(n_spaces,line_char) + \"]  \" \n",
    "\n",
    "    clear_output()\n",
    "    display(bar_txt + text_txt)\n",
    "    \n",
    "def vspace(k):\n",
    "    spaces = [\"\\n\" * k][0]\n",
    "    print(spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YFHQz-n-qv6O"
   },
   "source": [
    "\n",
    "# Motivational examples\n",
    "\n",
    "Recall from the last lecture that we experimented with fitting polynomial models to some noisy observations from a nonlinear function. We're going to approximate that function by fitting a polynomial to the observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Fitting a polynomial to a contiunous function (see 'Linear Models' notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "num_samples = 50\n",
    "np.random.seed(42)\n",
    "\n",
    "X = pd.DataFrame(np.sort(np.random.rand(num_samples)), columns=['x1'])\n",
    "Y = true_function(X.x1) + np.random.randn(num_samples) * 0.5\n",
    "functions = {\"True function\": true_function}\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "# degrees of the polynomial\n",
    "degrees = [1, 2, 4, 6, 8]\n",
    "for i in range(len(degrees)):\n",
    "    ax = plt.subplot(1, len(degrees), i + 1)\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "    plot_poly(X, Y, degrees[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well the fit certainly improved!\n",
    "So now what...Which polynomial should we as our model..?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivational example 2: Predicting wine quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8bm3k1wDqv7U"
   },
   "source": [
    "_\"All wines should be tasted; some should only be sipped, but with others, drink the whole bottle.\"_ - Paulo Coelho, Brida\n",
    "\n",
    "We will use a data set related to the red variant of the Portuguese \"Vinho Verde\" wine. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.). Our goal is to use machine learning to detect above-average wines (perhaps to send these wines later to professional tasters?).\n",
    "\n",
    "### Start by loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "df = pd.read_csv(url, delimiter=\";\").dropna()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform some basic data wrangling to clean up the column names and also look for duplicate records.\n",
    "\n",
    "The variable names have spaces in them which makes it more cumbersome to compute with them.  We can clean this up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_old    = df.columns\n",
    "df.columns  = [c.replace(' ', '_') for c in df.columns]\n",
    "pd.DataFrame({\"old\": cols_old, \"new\": df.columns}).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we've decided to check for duplicate records and, for purposes of this example, we will drop them.  (Why might we want to keep them instead?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_idx = df.duplicated(keep='first')\n",
    "sum(dup_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(keep=\"first\")\n",
    "sum(df.duplicated(keep='first'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up training and test data sets\n",
    "Next we need to divide the data into a training set and a testing set.  **{BEN: CHANGE TO URL FOR GIT}**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/bwolfson2/ds4biz22/blob/main/Module_3_Model_Performance_Analytics_I/images/sample_split.png?raw=1\" alt=\"Splitting the data\" style=\"width: 200px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "column_names      = df.columns   \n",
    "\n",
    "predictor_columns = column_names[:-1] # independent variable names\n",
    "y_column          = column_names[-1]\n",
    "\n",
    "X = df[[x for x in predictor_columns]]\n",
    "Y = df[y_column]\n",
    "\n",
    "# Splitting the data into train and test!\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note above the \"`pandas`\" way of doing things: process all the instances simultaneously computing the mean in one swoop; assigning the new column all at once.  This is a form of *vectorization*.\n",
    "\n",
    "Next we need to create a categorical variable from our raw target variable `quality`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_Y_train = np.mean(Y_train)  \n",
    "is_good = cp.copy(discretize_Y(Y_train, avg_Y_train).astype(\"int32\"))\n",
    "Y_train = cp.copy(is_good)\n",
    "Y_train = Y_train.rename(columns={'quality': 'is_good'})\n",
    "Y_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(54% of our instances are of \"good\" quality.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform some basic EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# create a data set to use for comparing distributions \n",
    "# and looking at correlations\n",
    "train_eda_dat = pd.concat([X_train, Y_train], axis=1)\n",
    "train_eda_dat.is_good.astype(\"int32\")\n",
    "train_eda_dat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "report  = create_report(train_eda_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can learn a bit more about our data from report above. \n",
    "\n",
    "Firstly, the target variable \"is_good\" does not appear to be not highly correlated with any of the dependent variables. \n",
    "\n",
    "There are also slight distributional skews in the variables, as well as in the interactions between `is_good` and other variables.  In a robust analysis, we would want  to consider various transformations, but for purposes of this example, we will skip that step. \n",
    "\n",
    "Finally, there are some high correlations among some of the variables.  We would normally need to deal with thhis too, but for the sake of the example (again!) we will pretend we don't see this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fitting a model to predict wine quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "\n",
    "def polynomial_model(model=LogisticRegression(), degree=1):\n",
    "    polynomial_features = PolynomialFeatures(degree=degree, include_bias=True)\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features), (\"model\", model)])\n",
    "    return pipeline\n",
    "\n",
    "def training_accy(X_train, Y_train,X_test,Y_test, model):\n",
    "    warnings.filterwarnings('ignore')\n",
    "    y_hat = model.fit(X_train, Y_train).predict(X_test)\n",
    "    return accuracy_score(Y_test, [1 if ty >= 0.5 else 0 for ty in y_hat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to set up our test data the same way.  Notice we **did not** use the test data for any EDA and we will use the preprocessing parameter (`avg_Y_train`) from the training data, rather than reestimating it on the training data.  (Why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_good = cp.copy(discretize_Y(Y_test, avg_Y_train).astype(\"int32\"))\n",
    "Y_test  = cp.copy(is_good)\n",
    "Y_test  = Y_test.rename(columns={'quality': 'is_good'})\n",
    "Y_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit a number of candidate models and compare their in- and out-of-sample performance.  To get started, we will compute the `accuracy_score` for each model on both in-sample and out-of-sample data sets.  Recall that the $$accuracy\\_score = \\sum_i I(pred_i = actual_i),$$\n",
    "where \n",
    "$$I(c) =  \\Large\\left[^{1: ~ c ~ is ~ TRUE, ~ and}_{0: ~otherwise}.~ \\right.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beepy import beep   # warning: this is an alpha package \n",
    "\n",
    "degrees   = [1, 2, 3, 4, 5, 10, 12]\n",
    "n_degrees = len(degrees)\n",
    "acc_in       =  np.zeros(n_degrees)\n",
    "acc_out      =  np.zeros(n_degrees)\n",
    "\n",
    "progress_bar(0, n_degrees, incr_txt = \"Finished polynomial fit\", bar_len = 20)\n",
    "\n",
    "for i in range(n_degrees):\n",
    "    model = polynomial_model(LogisticRegression(solver='liblinear',max_iter=1000), degrees[i])\n",
    "    model.fit(X_train, Y_train.values)\n",
    "    \n",
    "    y_hat        = model.predict(X_train)\n",
    "    accy         = accuracy_score(Y_train, [1 if ty >= 0.5 else 0 for ty in y_hat])\n",
    "    acc_in[i]    = accy\n",
    "    \n",
    "    y_hat        = model.predict(X_test)\n",
    "    accy         = accuracy_score(Y_test, [1 if ty >= 0.5 else 0 for ty in y_hat])\n",
    "    acc_out[i]   = accy\n",
    "\n",
    "    progress_bar(i, n_degrees, incr_txt = \"Polynomial fit\", bar_len = 20)\n",
    "beep(sound=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores  = pd.DataFrame(data={\"polynomial degree\":degrees, \n",
    "                             \"in-sample\":np.round(acc_in,2), \n",
    "                             \"out-of-sample\":np.round(acc_out,2), \n",
    "                             \"dif(i-o)\":np.round(acc_in-acc_out,2)\n",
    "                            })\n",
    "vspace(2) \n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the results?  What explanation might you give?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going beyond `accuracy_score`: Contingency tables and confusion matrices "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Direct mailing \n",
    "We're going to use a mail response data set from a real direct marketing campaign located in `data/mailing.csv`. Each record represents an individual who was targeted with a direct marketing offer.  The offer was a solicitation to make a charitable donation. \n",
    "\n",
    "The columns (features) are:\n",
    "\n",
    "| feature   | description |\n",
    "| :---      |        ---: | \n",
    "|`income`   | household income |\n",
    "|`Firstdate`| date assoc. with the first gift by this individual|\n",
    "|`Lastdate` | date associated with the most recent gift |\n",
    "|`Amount`   | average amount by this individual over all periods (incl. zeros)|\n",
    "|`rfaf2`    | frequency code|\n",
    "|`rfaa2`    | donation amount code|\n",
    "|`pepstrfl` | flag indicating a star donor|\n",
    "|`glast`    | amount of last gift|\n",
    "|`gavr`     | amount of average gift|\n",
    "\n",
    "\n",
    "\n",
    "The target variables is `class` and is equal to one if a donor made a contribution in the current campaign and zero otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(\"data/mailing.csv\")\n",
    "# Let's take a look at the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, ngood, nbad = count_classes(df['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDzQ2Ax2LvNm"
   },
   "source": [
    "## Data prep\n",
    "From the description above, and the head of the data, we see that two of the fields are **categorical** instead of typical **numerical** fields. In what follows, we will be estimating is a logistic model. From the previous classes, we have seen that logistic regression requires *all* fields to be numerical. So, we are going to create \"dummy\" variables for all the fields that are categorical (same as you did for your homework).  [I believe in sklearn all models require numeric variables; that's not the case for every implementation.]\n",
    "\n",
    "\n",
    "A dummy variable is a binary variable corresponding to one value of a categorical variable.\n",
    "\n",
    "The typical way to create dummies for a field is to create new variables for each possible category of the field. For example consider a field called color that can have the possible values \"red\", \"blue\", and \"green\". To dummyize color, we would create three new features: \"color_red\", \"color_blue\", and \"color_green\". These fields would take the value 1 or 0 depending on the actual value of color. Each record can only have one of these fields set to 1.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- You can also leave out one of the possible categories. For example, in the above example that had three possible values, you can create only two dummies. This, because when \"color_red\"=0 and \"color_blue\"=0 it means that \"color_green=1\".  Often all three dummies are created anyway; it is slightly redundant, but makes the models more comprehensible.\n",
    "\n",
    "- There also are cases where non-numeric variables can take on multiple values (for example, `colors = {red, white, blue}`).  In these cases again often binary variables are created for each value, the obvious difference being that now more than one can be non-zero (and you would need to represent all the values).\n",
    " \n",
    "\n",
    "So.  Let's dummyize the fields `rfaa2` and `pepstrfl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for field in ['rfaa2', 'pepstrfl']:\n",
    "    dummies = pd.get_dummies(df[field])\n",
    "    dummies.columns = [field + \"_\" + s for s in dummies.columns]\n",
    "    df = pd.concat([df, dummies], axis=1).drop(field, axis=\"columns\")\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_Yw7qSwLvNy"
   },
   "source": [
    "## Confusion matrices\n",
    "Let's build a confusion matrix using a logistic regression model. \n",
    "\n",
    "**Important and overlooked (always remember this!):** a confusion matrix is defined with respect to a classifier, not a scoring model.  However, our models *are* scoring models (e.g., class-probability estimation models).  So the confusion matrices are defined with respect a scoring model plus a *threshold* on the score.  The threshold should be chosen carefully, and with the business need in mind.   For binary classes, the default for most modeling programs when they return a predicted classification is to use a threshold corresponding to an estimated class probability of 0.5.  This is because the modeling program does not know the business setting, and 0.5 makes sense as a default (in expectation it gives the maximum classification accuracy, if the probabilities are well calibrated).\n",
    "\n",
    "In class we saw this in terms of a Bankruptcy Model example:\n",
    "\n",
    "<img src=\"file:confusion_matrix.png?raw=1\" alt=\"Schematic of Confusion Matrix\" style=\"width: 600px;\"/>\n",
    "<span style=\"font-size: 10%\"> <em>Stein, R. M., lecture notes, 2022</em></span>\n",
    "\n",
    "$ $\n",
    "$ $\n",
    "\n",
    "So let's start with the default of predicting a 1 if the estimated probability is $\\geq$ 50% and a 0 otherwise.\n",
    "\n",
    "Remember, a confusion matrix looks like:\n",
    "\n",
    "```\n",
    "  |____________ p __________|___________ n ___________|\n",
    "Y | 1's predicted to be 1's | 0's predicted to be 1's |\n",
    "N | 1's predicted to be 0's | 0's predicted to be 0's |\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Split our data into training and test sets\n",
    "X = df.drop(['class'], axis=1)\n",
    "Y = df['class']\n",
    "X_mailing_train, X_mailing_test, Y_mailing_train, Y_mailing_test = train_test_split(X, Y, test_size=.25, random_state=42)\n",
    "\n",
    "# Make and fit a model on the training data\n",
    "model_mailing = LogisticRegression(C=1000000, solver='liblinear')\n",
    "model_mailing.fit(X_mailing_train, Y_mailing_train)\n",
    "\n",
    "# Get probabilities of being a donor (We saw this in a prior last class)\n",
    "probabilities = model_mailing.predict_proba(X_mailing_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do some crude preprocessing on our independent variables to make it easier to work with the data.  We will need to do this again with the test data, so it is convenient to create a short function to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the default threshold of 50% to predict a 1.\n",
    "\n",
    "(An individual below this threshold will get a label \"0\" and someone above this will get a label \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = probabilities > 0.5\n",
    "\n",
    "# Build and print a confusion matrix\n",
    "confusion_matrix_p50 = pd.DataFrame(metrics.confusion_matrix(Y_mailing_test, prediction, labels=[1, 0]).T,\n",
    "                                 columns=['p', 'n'], index=['Y', 'N'])\n",
    "print_confusion_matrix(confusion_matrix_p50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztIEkLytLvOL"
   },
   "source": [
    "Wait -- take a close look at that.  What's going on?\n",
    "\n",
    "Incidentally, what would be the classification accuracy here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpXz9IJjLvON"
   },
   "source": [
    "What if we lower the threshold to 5%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's move the threshold down\n",
    "prediction = probabilities > 0.05\n",
    "\n",
    "# Build and print a confusion matrix\n",
    "confusion_matrix_p05 = pd.DataFrame(metrics.confusion_matrix(Y_mailing_test, prediction, labels=[1, 0]).T,\n",
    "                                columns=['p', 'n'], index=['Y', 'N'])\n",
    "print_confusion_matrix(confusion_matrix_p05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XT8Fl3iHLvOZ"
   },
   "source": [
    "***\n",
    "Is this good performance? \n",
    "\n",
    "How can we tell?\n",
    "\n",
    "(Incidentally, what would be the classification accuracy now?)\n",
    "\n",
    "Is 5% the right threshold?  How would we determine that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymJpg0gGLvOc"
   },
   "source": [
    "##  A more robust measure of discriminatory power: Receiver operating characteristic (ROC) curves\n",
    "\n",
    "In the book, we were trying to predict if customers should be given a credit card.  Let's use a version of that example here:\n",
    "\n",
    "- Target: `Y = 1` \n",
    "- Three features in `X_handson`: \"earning\", \"geographic\", and \"experience\".\n",
    "\n",
    "Up until this point, when we need a \"single number metric\" for generalization performance, we have been using \"vanilla\" classification accuracy (the number of records correctly classified divided by the total number of records). However, classification accuracy usually does not  give us the \"best\" interpretation of our model's performance for a particular business problem. An more general evaluation is to visualize and measure the performance of a model using the Reciever Operating Characteristic **(ROC) curve**. \n",
    "\n",
    "Let's first specify how we create ROC curves: For each threshold $t$ that is chosen, we can define two quantities. First, the false positive rate, $FPR = \\frac{False Positives}{False Positives+True Negatives}$, and second, the true positive rate, $TPR = \\frac{True Positives}{True Positives+False Negatives}$. The ROC curve is the result of plotting $FPR$ against $TPR$ for each value of $t$ that is possible in the data.  It helps us to visualize and analyze the trade-offs between the opportunity for benefits (via true positives) and the possibility of costs (via false positives). \n",
    "\n",
    "\" The lower left point **(0, 0) represents the strategy of never issuing a positive classification**; such a classifier commits no false positive errors but also gains no true positives. The opposite strategy, of unconditionally issuing positive classifications, is represented by the upper right point (1, 1). The point **(0, 1) represents perfect classification** (the star in Figure 8-3). The diagonal line connecting (0, 0) to (1, 1) represents the policy of guessing a class (for example, by flipping a weighted coin). \"  \n",
    "\n",
    "- _Provost, Foster, and Tom Fawcett. Data Science for Business: _\n",
    "  _What you need to know about data mining and data-analytic thinking. O'Reilly Media, Inc., 2013._\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/bwolfson2/foster_in_dev/blob/master/Module5_ROC_Cost_Visualization/images/ROC1.png?raw=1\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<img src=\"https://github.com/bwolfson2/foster_in_dev/blob/master/Module5_ROC_Cost_Visualization/images/ROC2.png?raw=1\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<img src=\"https://github.com/bwolfson2/foster_in_dev/blob/master/Module5_ROC_Cost_Visualization/images/ROC3.png?raw=1\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "Doing this in **sklearn** is relatively straightforward.\n",
    "\n",
    "Let's create a **new DATA SET** for this, that will show differences between different models.  We will build logistic regression models with different regularization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "plt.figure(figsize=(10, 10)) \n",
    "model_forms   = [\"Logistic\", \"Tree\", \"KNN\"]\n",
    "n_model_forms = len(model_forms)\n",
    "progress_bar(0, 3, incr_txt = model_forms[0], bar_len = 20)\n",
    "\n",
    "# Fit a logistic regression model\n",
    "for i, model in enumerate([LogisticRegression(C=.1, solver='liblinear'),\n",
    "                           DecisionTreeClassifier(),\n",
    "                           KNeighborsClassifier(n_neighbors=5)\n",
    "                          ]):\n",
    "    model.fit(X_mailing_train, Y_mailing_train)\n",
    "\n",
    "    # Get the probability of Y_test records being = 1\n",
    "    Y_test_probability_1 = model.predict_proba(X_mailing_test)[:, 1]\n",
    "\n",
    "    # Use the metrics.roc_curve function to get the true positive rate (tpr) and false positive rate (fpr)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(Y_mailing_test, Y_test_probability_1)\n",
    "\n",
    "    # Compute (estimate) the area under the curve (AUC)\n",
    "    auc = np.mean(cross_val_score(model, X, Y, scoring=\"roc_auc\", cv=5))\n",
    "\n",
    "    # Plot the ROC curve\n",
    "    label = f\"{model} AUC=({str(round(auc, 2))})\"\n",
    "    plt.plot(fpr, tpr, label = label)\n",
    "\n",
    "    progress_bar(i, n_model_forms, incr_txt = model_forms[i], bar_len = 20)\n",
    "\n",
    "progress_bar(n_model_forms, n_model_forms, incr_txt = \"Estimation/Evaluation\", bar_len = 20)\n",
    "plt.xlabel(\"False positive rate (fpr)\")\n",
    "plt.ylabel(\"True positive rate (tpr)\")\n",
    "plt.plot([0,1], [0,1], 'k--', label=\"Random\")\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based only on the the ROC curves above, which model would you choose?  Can you imagine a situation in which you would prefer one of the other models over the one you chose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class, we learned that one definition of the AUC is the probability a randomly selected \"good\" instance being ranked better than a randomly chosen \"bad\" instance.\n",
    "\n",
    "$$A = p(g~<~b),~ g \\in G, ~b \\in B$$\n",
    "or, empirically,\n",
    "$$\\hat(A)=\\frac{1}{mn}\\sum_j^n \\sum_i^m ~ I(G_j < B_j)$$\n",
    "\n",
    "Let's see if this is really the case.\n",
    "\n",
    "First, let's see how big the test sample is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, ngood, nbad = count_classes(Y_mailing_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This set of comparisons is large ($191,779$ positive $\\times 2,423$ negative $= 464,680,517$, and would took about a minute to calculate on a fast desktop computer.  \n",
    "\n",
    "But imagine if we had to calculate Equation (2) for a more evenly balanced data set, say $200,000$ bads and $200,000$ goods.  Calculating Equation (2) explicitly would require $40,000,000,000$ comparisons, or about 1:30 hours. If we had an evenly balanced data set with only $2,000,000$ records in total, this would be would require about six days.   \n",
    "\n",
    "Fortunately, we can approximate Equation (2) by sampling the data, rather than using all of it.  In this case, this would mean taking a random sample of the contributors and a random sample of non-contributors and then compare them instance-wise to calculate Equation (2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choices\n",
    "\n",
    "np.random.seed(61)\n",
    "\n",
    "n_samples = 100000\n",
    "\n",
    "GLM_mailing   = LogisticRegression(C=1000000, solver='liblinear')\n",
    "GLM_mailing.fit(X_mailing_train, Y_mailing_train)\n",
    "probabilities = GLM_mailing.predict_proba(X_mailing_test)[:, 1]\n",
    "\n",
    "GLM_auc = np.mean(cross_val_score(GLM_mailing, X_mailing_test, Y_mailing_test, scoring=\"roc_auc\", cv=5))\n",
    "\n",
    "G = probabilities[Y_mailing_test == 1]\n",
    "B = probabilities[Y_mailing_test == 0]\n",
    "\n",
    "G_sample = choices(G, k = n_samples)\n",
    "B_sample = choices(B, k = n_samples)\n",
    "\n",
    "n_G_less_than_B = [0 if G_sample[i] < B_sample[i] else 1 for i in range(n_samples)]\n",
    "GLM_auc_samp = np.sum(n_G_less_than_B)/n_samples\n",
    "\n",
    "print(\"{:>12} : {:>6}\".format(\"Full AUC\", round(GLM_auc,4)))\n",
    "print(\"{:>12} : {:>6}\".format(\"Sample AUC\", round(GLM_auc_samp,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two results are, for practical purposes, identical."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "389.0625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
